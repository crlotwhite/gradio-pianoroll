#!/usr/bin/env python3
"""
Audio Features Demo: Multi-Feature Audio Analysis
Description: F0, loudness, voicing ë“± ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŠ¹ì„±ì„ ë™ì‹œì— ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
Author: gradio-pianoroll
"""

import gradio as gr
import numpy as np
import io
import base64
import wave
import tempfile
import os
from gradio_pianoroll import PianoRoll

# librosa ì¶”ê°€ ì„í¬íŠ¸
try:
    import librosa
    LIBROSA_AVAILABLE = True
    print("âœ… librosa ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    LIBROSA_AVAILABLE = False
    print("âš ï¸ librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# í•©ì„±ê¸° ì„¤ì • (ì˜¤ë””ì˜¤ ìƒì„±ìš©)
SAMPLE_RATE = 44100
MAX_DURATION = 10.0  # ìµœëŒ€ 10ì´ˆ

def midi_to_frequency(midi_note):
    """MIDI ë…¸íŠ¸ ë²ˆí˜¸ë¥¼ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜ (A4 = 440Hz)"""
    return 440.0 * (2.0 ** ((midi_note - 69) / 12.0))

def generate_sine_wave(frequency, duration, sample_rate):
    """ì‚¬ì¸íŒŒ ìƒì„±"""
    t = np.linspace(0, duration, int(duration * sample_rate), False)
    return np.sin(2 * np.pi * frequency * t)

def synthesize_simple_audio(piano_roll_data):
    """ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ í•©ì„± (íŠ¹ì„± ë¶„ì„ìš©)"""
    if not piano_roll_data or 'notes' not in piano_roll_data or not piano_roll_data['notes']:
        return None

    notes = piano_roll_data['notes']
    tempo = piano_roll_data.get('tempo', 120)
    pixels_per_beat = piano_roll_data.get('pixelsPerBeat', 80)

    # ì „ì²´ ê¸¸ì´ ê³„ì‚°
    max_end_time = 0
    for note in notes:
        start_seconds = (note['start'] / pixels_per_beat) * (60.0 / tempo)
        duration_seconds = (note['duration'] / pixels_per_beat) * (60.0 / tempo)
        end_time = start_seconds + duration_seconds
        max_end_time = max(max_end_time, end_time)

    total_duration = min(max_end_time + 0.5, MAX_DURATION)
    total_samples = int(total_duration * SAMPLE_RATE)
    audio_buffer = np.zeros(total_samples)

    # ê° ë…¸íŠ¸ ì²˜ë¦¬ (ê°„ë‹¨í•œ ì‚¬ì¸íŒŒ)
    for note in notes:
        pitch = note['pitch']
        velocity = note.get('velocity', 100)
        
        start_seconds = (note['start'] / pixels_per_beat) * (60.0 / tempo)
        duration_seconds = (note['duration'] / pixels_per_beat) * (60.0 / tempo)
        
        if start_seconds >= total_duration:
            continue
            
        if start_seconds + duration_seconds > total_duration:
            duration_seconds = total_duration - start_seconds
            
        if duration_seconds <= 0:
            continue

        frequency = midi_to_frequency(pitch)
        volume = velocity / 127.0 * 0.3  # ë³¼ë¥¨ ì¡°ì •
        
        # ê°„ë‹¨í•œ ì‚¬ì¸íŒŒ ìƒì„±
        wave = generate_sine_wave(frequency, duration_seconds, SAMPLE_RATE)
        
        # ì˜¤ë””ì˜¤ ë²„í¼ì— ì¶”ê°€
        start_sample = int(start_seconds * SAMPLE_RATE)
        end_sample = start_sample + len(wave)
        
        if start_sample < total_samples:
            end_sample = min(end_sample, total_samples)
            audio_length = end_sample - start_sample
            if audio_length > 0:
                audio_buffer[start_sample:end_sample] += wave[:audio_length] * volume

    # ì •ê·œí™”
    max_amplitude = np.max(np.abs(audio_buffer))
    if max_amplitude > 0:
        audio_buffer = audio_buffer / max_amplitude * 0.9

    return audio_buffer

def create_temp_wav_file(audio_data, sample_rate):
    """ì„ì‹œ WAV íŒŒì¼ ìƒì„±"""
    if audio_data is None or len(audio_data) == 0:
        return None

    try:
        audio_16bit = (audio_data * 32767).astype(np.int16)
        temp_fd, temp_path = tempfile.mkstemp(suffix='.wav')

        with wave.open(temp_path, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_16bit.tobytes())

        os.close(temp_fd)
        return temp_path
    except Exception as e:
        print(f"ì„ì‹œ WAV íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def extract_audio_features(audio_file_path, f0_method="pyin", include_f0=True, include_loudness=True, include_voicing=True):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ F0, loudness, voice/unvoice ì¶”ì¶œ"""
    if not LIBROSA_AVAILABLE:
        return None, "librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

    features = {}
    status_messages = []

    try:
        print(f"ğŸµ ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ì‹œì‘: {audio_file_path}")

        # ì˜¤ë””ì˜¤ ë¡œë“œ
        y, sr = librosa.load(audio_file_path, sr=None)
        print(f"   - ìƒ˜í”Œë ˆì´íŠ¸: {sr}Hz")
        print(f"   - ê¸¸ì´: {len(y)/sr:.2f}ì´ˆ")

        hop_length = 512

        # F0 ì¶”ì¶œ
        if include_f0:
            try:
                if f0_method == "pyin":
                    f0, voiced_flag, voiced_probs = librosa.pyin(
                        y, sr=sr,
                        fmin=librosa.note_to_hz('C2'),
                        fmax=librosa.note_to_hz('C7')
                    )
                else:
                    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
                    f0 = []
                    for t in range(pitches.shape[1]):
                        index = magnitudes[:, t].argmax()
                        pitch = pitches[index, t]
                        f0.append(pitch if pitch > 0 else np.nan)
                    f0 = np.array(f0)
                    voiced_flag = ~np.isnan(f0)
                    voiced_probs = voiced_flag.astype(float)

                frame_times = librosa.frames_to_time(np.arange(len(f0)), sr=sr, hop_length=hop_length)

                features['f0'] = {
                    'times': frame_times,
                    'f0_values': f0,
                    'voiced_flag': voiced_flag,
                    'voiced_probs': voiced_probs,
                    'sample_rate': sr,
                    'duration': len(y) / sr,
                    'hop_length': hop_length
                }
                status_messages.append("F0 ì¶”ì¶œ ì™„ë£Œ")
                
                valid_f0 = f0[~np.isnan(f0)]
                if len(valid_f0) > 0:
                    print(f"   - F0 ë²”ìœ„: {np.min(valid_f0):.1f}Hz ~ {np.max(valid_f0):.1f}Hz")
                
            except Exception as e:
                status_messages.append(f"F0 ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        # Loudness ì¶”ì¶œ
        if include_loudness:
            try:
                rms_energy = librosa.feature.rms(y=y, hop_length=hop_length)[0]
                frame_times = librosa.frames_to_time(np.arange(len(rms_energy)), sr=sr, hop_length=hop_length)

                # dBë¡œ ë³€í™˜
                max_rms = np.max(rms_energy)
                if max_rms > 0:
                    loudness_db = 20 * np.log10(rms_energy / max_rms)
                    loudness_db = np.maximum(loudness_db, -60)
                else:
                    loudness_db = np.full_like(rms_energy, -60)

                # 0-1ë¡œ ì •ê·œí™”
                loudness_normalized = (loudness_db + 60) / 60

                features['loudness'] = {
                    'times': frame_times,
                    'rms_values': rms_energy,
                    'loudness_db': loudness_db,
                    'loudness_normalized': loudness_normalized,
                    'sample_rate': sr,
                    'duration': len(y) / sr,
                    'hop_length': hop_length
                }
                status_messages.append("Loudness ì¶”ì¶œ ì™„ë£Œ")
                print(f"   - RMS ë²”ìœ„: {np.min(rms_energy):.6f} ~ {np.max(rms_energy):.6f}")
                
            except Exception as e:
                status_messages.append(f"Loudness ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        # Voice/Unvoice ì¶”ì¶œ
        if include_voicing and 'f0' in features:
            try:
                voicing_data = features['f0']  # F0ì—ì„œ voiced ì •ë³´ ì‚¬ìš©
                voiced_ratio = np.sum(voicing_data['voiced_flag']) / len(voicing_data['voiced_flag'])
                
                features['voicing'] = {
                    'times': voicing_data['times'],
                    'voiced_flag': voicing_data['voiced_flag'],
                    'voiced_probs': voicing_data['voiced_probs'],
                    'sample_rate': sr,
                    'duration': len(y) / sr,
                    'hop_length': hop_length,
                    'voiced_ratio': voiced_ratio
                }
                status_messages.append("Voice/Unvoice ì¶”ì¶œ ì™„ë£Œ")
                print(f"   - Voiced ë¹„ìœ¨: {voiced_ratio:.1%}")
                
            except Exception as e:
                status_messages.append(f"Voice/Unvoice ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        if features:
            features['duration'] = len(y) / sr
            features['sample_rate'] = sr
            return features, " | ".join(status_messages)
        else:
            return None, "ëª¨ë“  íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨"

    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None, f"ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

def create_multi_feature_line_data(features, tempo=120, pixels_per_beat=80):
    """ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŠ¹ì„±ì„ ê²°í•©í•˜ì—¬ line_data ìƒì„±"""
    combined_line_data = {}

    try:
        # F0 ê³¡ì„  ì¶”ê°€
        if 'f0' in features:
            f0_data = features['f0']
            times = f0_data['times']
            f0_values = f0_data['f0_values']

            # í”¼ì•„ë…¸ë¡¤ ìƒìˆ˜
            NOTE_HEIGHT = 20
            TOTAL_NOTES = 128

            def hz_to_midi(frequency):
                if frequency <= 0:
                    return 0
                return 69 + 12 * np.log2(frequency / 440.0)

            def midi_to_y_coordinate(midi_note):
                return (TOTAL_NOTES - 1 - midi_note) * NOTE_HEIGHT + NOTE_HEIGHT/2

            # F0 ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
            f0_points = []
            valid_f0_values = []

            for time, f0 in zip(times, f0_values):
                if not np.isnan(f0) and f0 > 0:
                    midi_note = hz_to_midi(f0)
                    if 0 <= midi_note <= 127:
                        x_pixel = time * (tempo / 60) * pixels_per_beat
                        y_pixel = midi_to_y_coordinate(midi_note)
                        f0_points.append({"x": float(x_pixel), "y": float(y_pixel)})
                        valid_f0_values.append(f0)

            if f0_points:
                min_f0 = float(np.min(valid_f0_values))
                max_f0 = float(np.max(valid_f0_values))
                
                combined_line_data['f0_curve'] = {
                    "color": "#FF6B6B",
                    "lineWidth": 3,
                    "yMin": 0,
                    "yMax": TOTAL_NOTES * NOTE_HEIGHT,
                    "position": "overlay",
                    "renderMode": "piano_grid",
                    "visible": True,
                    "opacity": 0.8,
                    "data": f0_points,
                    "dataType": "f0",
                    "unit": "Hz",
                    "originalRange": {
                        "minHz": min_f0,
                        "maxHz": max_f0,
                        "minMidi": hz_to_midi(min_f0),
                        "maxMidi": hz_to_midi(max_f0)
                    }
                }

        # Loudness ê³¡ì„  ì¶”ê°€
        if 'loudness' in features:
            loudness_data = features['loudness']
            times = loudness_data['times']
            loudness_db = loudness_data['loudness_db']

            loudness_points = []
            for time, value in zip(times, loudness_db):
                if not np.isnan(value):
                    x_pixel = time * (tempo / 60) * pixels_per_beat
                    # -60dB ~ 0dBë¥¼ 0 ~ 2560 í”½ì…€ë¡œ ë³€í™˜
                    normalized_value = (value + 60) / 60
                    y_pixel = normalized_value * 2560
                    loudness_points.append({"x": float(x_pixel), "y": float(max(0, min(2560, y_pixel)))})

            if loudness_points:
                combined_line_data['loudness_curve'] = {
                    "color": "#4ECDC4",
                    "lineWidth": 2,
                    "yMin": 0,
                    "yMax": 2560,
                    "position": "overlay",
                    "renderMode": "independent_range",
                    "visible": True,
                    "opacity": 0.6,
                    "data": loudness_points,
                    "dataType": "loudness",
                    "unit": "dB",
                    "originalRange": {
                        "min": float(np.min(loudness_db)),
                        "max": float(np.max(loudness_db)),
                        "y_min": -60,
                        "y_max": 0
                    }
                }

        # Voice/Unvoice ê³¡ì„  ì¶”ê°€
        if 'voicing' in features:
            voicing_data = features['voicing']
            times = voicing_data['times']
            voiced_probs = voicing_data['voiced_probs']

            voicing_points = []
            for time, value in zip(times, voiced_probs):
                if not np.isnan(value):
                    x_pixel = time * (tempo / 60) * pixels_per_beat
                    y_pixel = value * 2560  # 0-1ì„ 0-2560 í”½ì…€ë¡œ ë³€í™˜
                    voicing_points.append({"x": float(x_pixel), "y": float(max(0, min(2560, y_pixel)))})

            if voicing_points:
                combined_line_data['voicing_curve'] = {
                    "color": "#9B59B6",
                    "lineWidth": 2,
                    "yMin": 0,
                    "yMax": 2560,
                    "position": "overlay",
                    "renderMode": "independent_range",
                    "visible": True,
                    "opacity": 0.6,
                    "data": voicing_points,
                    "dataType": "voicing",
                    "unit": "probability",
                    "originalRange": {
                        "min": 0.0,
                        "max": 1.0,
                        "voiced_ratio": voicing_data.get('voiced_ratio', 0.0),
                        "y_min": 0,
                        "y_max": 1
                    }
                }

        if combined_line_data:
            print(f"ğŸ“Š ê²°í•©ëœ LineData ìƒì„±ë¨: {len(combined_line_data)} ê³¡ì„ ")
            return combined_line_data
        else:
            print("âš ï¸ ê³¡ì„  ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            return None

    except Exception as e:
        print(f"âŒ ê²°í•©ëœ LineData ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def analyze_generated_audio_features(piano_roll, include_f0=True, include_loudness=True, include_voicing=True, f0_method="pyin"):
    """ë…¸íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•˜ê³  íŠ¹ì„± ë¶„ì„"""
    print("=== ìƒì„±ëœ ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ===")

    # ì˜¤ë””ì˜¤ í•©ì„±
    audio_data = synthesize_simple_audio(piano_roll)
    if audio_data is None:
        return piano_roll, "ì˜¤ë””ì˜¤ í•©ì„± ì‹¤íŒ¨", None

    # ì„ì‹œ WAV íŒŒì¼ ìƒì„±
    temp_audio_path = create_temp_wav_file(audio_data, SAMPLE_RATE)
    if temp_audio_path is None:
        return piano_roll, "ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨", None

    try:
        # ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„
        features, analysis_status = extract_audio_features(
            temp_audio_path, f0_method, include_f0, include_loudness, include_voicing
        )

        if features is None:
            return piano_roll, f"ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {analysis_status}", temp_audio_path

        # í”¼ì•„ë…¸ë¡¤ ì—…ë°ì´íŠ¸
        updated_piano_roll = piano_roll.copy() if piano_roll else {}

        # ê³¡ì„  ë°ì´í„° ìƒì„±
        tempo = updated_piano_roll.get('tempo', 120)
        pixels_per_beat = updated_piano_roll.get('pixelsPerBeat', 80)

        line_data = create_multi_feature_line_data(features, tempo, pixels_per_beat)

        if line_data:
            updated_piano_roll['line_data'] = line_data

        # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
        status_parts = [f"ì˜¤ë””ì˜¤ í•©ì„± ì™„ë£Œ", analysis_status]

        if line_data:
            curve_count = len(line_data)
            status_parts.append(f"{curve_count}ê°œ íŠ¹ì„± ê³¡ì„  ì‹œê°í™” ì™„ë£Œ")

            # ê° íŠ¹ì„± ë²”ìœ„ ì •ë³´ ì¶”ê°€
            for curve_name, curve_info in line_data.items():
                if 'originalRange' in curve_info:
                    range_info = curve_info['originalRange']
                    if 'minHz' in range_info:  # F0
                        status_parts.append(f"F0: {range_info['minHz']:.1f}~{range_info['maxHz']:.1f}Hz")
                    elif 'min' in range_info and 'voiced_ratio' not in range_info:  # Loudness
                        unit = curve_info.get('unit', '')
                        status_parts.append(f"Loudness: {range_info['min']:.1f}~{range_info['max']:.1f}{unit}")
                    elif 'voiced_ratio' in range_info:  # Voice/Unvoice
                        voiced_ratio = range_info['voiced_ratio']
                        status_parts.append(f"Voicing: {voiced_ratio:.1%} voiced")

        duration = features.get('duration', 0)
        status_parts.append(f"â±ï¸ {duration:.2f}ì´ˆ")

        status_message = " | ".join(status_parts)

        return updated_piano_roll, status_message, temp_audio_path

    except Exception as e:
        error_message = f"íŠ¹ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"âŒ {error_message}")
        return piano_roll, error_message, temp_audio_path

def analyze_uploaded_audio_features(piano_roll, audio_file, include_f0=True, include_loudness=True, include_voicing=True, f0_method="pyin"):
    """ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ íŠ¹ì„± ë¶„ì„"""
    print("=== ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ===")
    print(f"ì˜¤ë””ì˜¤ íŒŒì¼: {audio_file}")

    if not audio_file:
        return piano_roll, "ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.", None

    if not LIBROSA_AVAILABLE:
        return piano_roll, "librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install librosa'ë¡œ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”.", None

    try:
        # ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„
        features, analysis_status = extract_audio_features(
            audio_file, f0_method, include_f0, include_loudness, include_voicing
        )

        if features is None:
            return piano_roll, f"ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {analysis_status}", audio_file

        # í”¼ì•„ë…¸ë¡¤ ë°ì´í„° ì—…ë°ì´íŠ¸
        updated_piano_roll = piano_roll.copy() if piano_roll else {
            "notes": [],
            "tempo": 120,
            "timeSignature": {"numerator": 4, "denominator": 4},
            "editMode": "select",
            "snapSetting": "1/4",
            "pixelsPerBeat": 80
        }

        # ê³¡ì„  ë°ì´í„° ìƒì„±
        tempo = updated_piano_roll.get('tempo', 120)
        pixels_per_beat = updated_piano_roll.get('pixelsPerBeat', 80)

        line_data = create_multi_feature_line_data(features, tempo, pixels_per_beat)

        if line_data:
            updated_piano_roll['line_data'] = line_data

        # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
        status_parts = [analysis_status]

        if line_data:
            curve_count = len(line_data)
            curve_types = list(line_data.keys())
            status_parts.append(f"{curve_count}ê°œ ê³¡ì„  ({', '.join(curve_types)}) ì‹œê°í™” ì™„ë£Œ")

            # ê° íŠ¹ì„± ë²”ìœ„ ì •ë³´ ì¶”ê°€
            for curve_name, curve_info in line_data.items():
                if 'originalRange' in curve_info:
                    range_info = curve_info['originalRange']
                    if 'minHz' in range_info:  # F0
                        status_parts.append(f"F0: {range_info['minHz']:.1f}~{range_info['maxHz']:.1f}Hz")
                    elif 'min' in range_info and 'voiced_ratio' not in range_info:  # Loudness
                        unit = curve_info.get('unit', '')
                        status_parts.append(f"Loudness: {range_info['min']:.1f}~{range_info['max']:.1f}{unit}")
                    elif 'voiced_ratio' in range_info:  # Voice/Unvoice
                        voiced_ratio = range_info['voiced_ratio']
                        status_parts.append(f"Voicing: {voiced_ratio:.1%} voiced")

        duration = features.get('duration', 0)
        status_parts.append(f"â±ï¸ {duration:.2f}ì´ˆ")

        status_message = " | ".join(status_parts)

        return updated_piano_roll, status_message, audio_file

    except Exception as e:
        error_message = f"ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"âŒ {error_message}")
        return piano_roll, error_message, audio_file

def generate_feature_demo_audio():
    """íŠ¹ì„± ë¶„ì„ ë°ëª¨ìš© ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ê°€ì§„ ì˜¤ë””ì˜¤ ìƒì„±"""
    print("ğŸµ íŠ¹ì„± ë¶„ì„ìš© ë°ëª¨ ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...")

    duration = 4.0  # 4ì´ˆ
    sample_rate = 44100
    t = np.linspace(0, duration, int(duration * sample_rate), False)

    # ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ê°€ì§„ ì˜¤ë””ì˜¤ ìƒì„±
    audio = np.zeros_like(t)

    # êµ¬ê°„ 1 (0-1ì´ˆ): C4ì—ì„œ C5ë¡œ + ë³¼ë¥¨ ì¦ê°€
    mask1 = (t >= 0) & (t < 1)
    t1 = t[mask1]
    f1_start, f1_end = 261.63, 523.25  # C4ì—ì„œ C5
    freq1 = f1_start + (f1_end - f1_start) * (t1 / 1.0)
    phase1 = 2 * np.pi * np.cumsum(freq1) / sample_rate
    vol1 = 0.1 + 0.4 * (t1 / 1.0)  # 0.1ì—ì„œ 0.5ë¡œ ì¦ê°€
    audio[mask1] = vol1 * np.sin(phase1)

    # êµ¬ê°„ 2 (1-2ì´ˆ): C5ì—ì„œ G4ë¡œ + ì¼ì • ë³¼ë¥¨
    mask2 = (t >= 1) & (t < 2)
    t2 = t[mask2] - 1
    f2_start, f2_end = 523.25, 392.00  # C5ì—ì„œ G4
    freq2 = f2_start + (f2_end - f2_start) * (t2 / 1.0)
    phase2 = 2 * np.pi * np.cumsum(freq2) / sample_rate
    audio[mask2] = 0.5 * np.sin(phase2)

    # êµ¬ê°„ 3 (2-3ì´ˆ): A4 ê³ ì • + ë³¼ë¥¨ ê°ì†Œ (íŠ¸ë ˆëª°ë¡œ íš¨ê³¼)
    mask3 = (t >= 2) & (t < 3)
    t3 = t[mask3] - 2
    freq3 = 440.0  # A4 ê³ ì •
    phase3 = 2 * np.pi * freq3 * t3
    vol3 = 0.5 * (1 - t3 / 1.0) * (1 + 0.3 * np.sin(2 * np.pi * 6 * t3))  # íŠ¸ë ˆëª°ë¡œ
    audio[mask3] = vol3 * np.sin(phase3)

    # êµ¬ê°„ 4 (3-4ì´ˆ): ë³µí•©ìŒ (A4 + E5) + í˜ì´ë“œ ì•„ì›ƒ
    mask4 = (t >= 3) & (t < 4)
    t4 = t[mask4] - 3
    freq4a, freq4b = 440.0, 659.25  # A4 + E5
    phase4a = 2 * np.pi * freq4a * t4
    phase4b = 2 * np.pi * freq4b * t4
    vol4 = 0.4 * (1 - t4 / 1.0)  # í˜ì´ë“œ ì•„ì›ƒ
    audio[mask4] = vol4 * (0.6 * np.sin(phase4a) + 0.4 * np.sin(phase4b))

    # WAV íŒŒì¼ë¡œ ì €ì¥
    temp_fd, temp_path = tempfile.mkstemp(suffix='.wav')
    try:
        with wave.open(temp_path, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)

            audio_16bit = (audio * 32767).astype(np.int16)
            wav_file.writeframes(audio_16bit.tobytes())

        os.close(temp_fd)
        print(f"âœ… íŠ¹ì„± ë¶„ì„ìš© ë°ëª¨ ì˜¤ë””ì˜¤ ìƒì„±ë¨: {temp_path}")
        return temp_path

    except Exception as e:
        os.close(temp_fd)
        print(f"âŒ íŠ¹ì„± ë¶„ì„ìš© ë°ëª¨ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# Gradio ì¸í„°í˜ì´ìŠ¤
with gr.Blocks(title="PianoRoll Audio Features Demo") as demo:
    gr.Markdown("# ğŸ”Š PianoRoll Audio Features Demo")
    if LIBROSA_AVAILABLE:
        gr.Markdown("ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  F0, loudness ë“± ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ë¶„ì„í•´ë³´ì„¸ìš”!")
    else:
        gr.Markdown("âš ï¸ **librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ**: `pip install librosa`ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”.")

    with gr.Row():
        with gr.Column(scale=3):
            # ì´ˆê¸°ê°’ (ë¹ˆ í”¼ì•„ë…¸ë¡¤)
            initial_value = {
                "notes": [],
                "tempo": 120,
                "timeSignature": {"numerator": 4, "denominator": 4},
                "editMode": "select",
                "snapSetting": "1/4",
                "pixelsPerBeat": 80
            }
            piano_roll = PianoRoll(
                height=600,
                width=1000,
                value=initial_value,
                elem_id="piano_roll_features"
            )

        with gr.Column(scale=1):
            gr.Markdown("### ğŸ¤ ì˜¤ë””ì˜¤ ì—…ë¡œë“œ")

            audio_input = gr.Audio(
                label="ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼",
                type="filepath",
                interactive=True
            )

            gr.Markdown("### âš™ï¸ ë¶„ì„ ì„¤ì •")
            f0_method_dropdown = gr.Dropdown(
                choices=[
                    ("PYIN (ì •í™•, ëŠë¦¼)", "pyin"),
                    ("PipTrack (ë¹ ë¦„, ë¶€ì •í™•)", "piptrack")
                ],
                value="pyin",
                label="F0 ì¶”ì¶œ ë°©ë²•"
            )

            btn_analyze = gr.Button(
                "ğŸ”¬ íŠ¹ì„± ë¶„ì„ ì‹œì‘",
                variant="primary",
                size="lg",
                interactive=LIBROSA_AVAILABLE
            )

    with gr.Row():
        status_text = gr.Textbox(
            label="ë¶„ì„ ìƒíƒœ",
            interactive=False,
            lines=4
        )

    with gr.Row():
        reference_audio = gr.Audio(
            label="ì›ë³¸ ì˜¤ë””ì˜¤ (ì°¸ì¡°ìš©)",
            type="filepath",
            interactive=False
        )

    output_json = gr.JSON(label="íŠ¹ì„± ë¶„ì„ ê²°ê³¼")

    # ì´ë²¤íŠ¸ ì²˜ë¦¬
    def simple_analyze_features(piano_roll, audio_file, f0_method):
        """ê°„ë‹¨í•œ ë¶„ì„ ë˜í¼"""
        return analyze_uploaded_audio_features(
            piano_roll, audio_file, 
            include_f0=True, include_loudness=True, include_voicing=False, 
            f0_method=f0_method
        )
    
    btn_analyze.click(
        fn=simple_analyze_features,
        inputs=[piano_roll, audio_input, f0_method_dropdown],
        outputs=[piano_roll, status_text, reference_audio],
        show_progress=True
    )

    # JSON ì¶œë ¥ ì—…ë°ì´íŠ¸
    piano_roll.change(
        fn=lambda x: x,
        inputs=[piano_roll],
        outputs=[output_json],
        show_progress=False
    )

if __name__ == "__main__":
    demo.launch() 