# Audio Utils API

ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í”¼ì•„ë…¸ë¡¤ ì¢Œí‘œê³„ë¡œ ë³€í™˜í•˜ê³  ì»¤ë¸Œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤. numpy ë°°ì—´, ë¦¬ìŠ¤íŠ¸, ë‹¨ì¼ ê°’ ë“± ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ì§€ì›í•˜ì—¬ AI/ML ì „ë¬¸ê°€ë“¤ì´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì„¤ì¹˜ ë° ì„í¬íŠ¸

```python
from gradio_pianoroll import (
    PianoRoll,
    PianoRollBackendData,
    Note,
    hz_to_pixels,
    time_to_pixels,
    hz_and_time_to_pixels,
    create_pitch_curve_data,
    create_loudness_curve_data,
    audio_array_to_base64_wav,
    librosa_f0_to_pixels,
    librosa_rms_to_pixels
)
```

## ì£¼ìš” í•¨ìˆ˜ ê°œìš”

| í•¨ìˆ˜ëª… | ì„¤ëª… | ì…ë ¥ | ì¶œë ¥ |
|--------|------|------|------|
| `hz_to_pixels` | Hzë¥¼ Yì¢Œí‘œë¡œ ë³€í™˜ | ì£¼íŒŒìˆ˜(Hz) | Yì¢Œí‘œ(í”½ì…€) |
| `time_to_pixels` | ì‹œê°„ì„ Xì¢Œí‘œë¡œ ë³€í™˜ | ì‹œê°„(ì´ˆ) | Xì¢Œí‘œ(í”½ì…€) |
| `create_pitch_curve_data` | F0 ì»¤ë¸Œ ë°ì´í„° ìƒì„± | ì£¼íŒŒìˆ˜+ì‹œê°„ ë°°ì—´ | ì™„ì „í•œ ì»¤ë¸Œ ë°ì´í„° |
| `create_loudness_curve_data` | ë¼ìš°ë“œë‹ˆìŠ¤ ì»¤ë¸Œ ìƒì„± | ìŒëŸ‰+ì‹œê°„ ë°°ì—´ | ì™„ì „í•œ ì»¤ë¸Œ ë°ì´í„° |
| `audio_array_to_base64_wav` | ì˜¤ë””ì˜¤ ë°°ì—´ì„ WAVë¡œ ë³€í™˜ | numpy ë°°ì—´ | base64 ë¬¸ìì—´ |

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘

### ê°„ë‹¨í•œ Hz ë³€í™˜

```python
import numpy as np
from gradio_pianoroll import hz_to_pixels, time_to_pixels

# ë‹¨ì¼ ê°’ ë³€í™˜
a4_y = hz_to_pixels(440.0)  # A4 â†’ 1290.0 pixels

# ë°°ì—´ ë³€í™˜ (numpy, list ëª¨ë‘ ì§€ì›)
freqs = [440.0, 523.25, 659.25]  # A4, C5, E5
y_coords = hz_to_pixels(freqs)
print(y_coords)  # [1290.0, 1230.0, 1170.0]

# ì‹œê°„ ë³€í™˜
times = [0.5, 1.0, 1.5]  # ì´ˆ
x_coords = time_to_pixels(times, tempo=120)
print(x_coords)  # [80.0, 160.0, 240.0]
```

### librosaì™€ í•¨ê»˜ ì‚¬ìš©

```python
import librosa
from gradio_pianoroll import create_pitch_curve_data, PianoRoll

# ì˜¤ë””ì˜¤ ë¶„ì„
y, sr = librosa.load("audio.wav")
f0, voiced_flag, voiced_probs = librosa.pyin(y, sr=sr)

# ì‹œê°„ì¶• ê³„ì‚°
times = librosa.frames_to_time(np.arange(len(f0)), sr=sr)

# ìœ íš¨í•œ F0ë§Œ í•„í„°ë§
valid_mask = voiced_flag & (f0 > 0)
valid_f0 = f0[valid_mask]
valid_times = times[valid_mask]

# í”¼ì¹˜ ì»¤ë¸Œ ìƒì„±
pitch_curve = create_pitch_curve_data(valid_f0, valid_times)

# ìƒˆë¡œìš´ ë°©ì‹: ë°±ì—”ë“œ ë°ì´í„° ì‚¬ìš©
backend_data = PianoRollBackendData()
backend_data.add_curve("f0_curve", pitch_curve)

piano_roll = PianoRoll(
    value={"notes": []},
    backend_data=backend_data
)

# ê¸°ì¡´ ë°©ì‹ë„ ì—¬ì „íˆ ì§€ì›
piano_roll_legacy = PianoRoll(value={
    "notes": [],
    "line_data": {"f0_curve": pitch_curve}
})
```

## ğŸ“š ìƒì„¸ API ë¬¸ì„œ

### hz_to_pixels()

**Signature:**
```python
hz_to_pixels(frequency: Union[float, np.ndarray, List[float]]) -> Union[float, np.ndarray]
```

ì£¼íŒŒìˆ˜(Hz)ë¥¼ í”¼ì•„ë…¸ë¡¤ Yì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

**Parameters:**
- `frequency`: ì£¼íŒŒìˆ˜ ê°’ (Hz) - ë‹¨ì¼ê°’, ë¦¬ìŠ¤íŠ¸, numpy ë°°ì—´ ëª¨ë‘ ì§€ì›

**Returns:**
- ì…ë ¥ê³¼ ê°™ì€ íƒ€ì…ì˜ Yì¢Œí‘œ ê°’ (í”½ì…€)

**Examples:**
```python
# ìŒê³„ ë³€í™˜
notes = {
    'C4': 261.63, 'D4': 293.66, 'E4': 329.63, 'F4': 349.23,
    'G4': 392.00, 'A4': 440.00, 'B4': 493.88, 'C5': 523.25
}

for note_name, freq in notes.items():
    y_pos = hz_to_pixels(freq)
    print(f"{note_name}: {freq:.2f}Hz â†’ Y={y_pos:.1f}px")

# NumPy ë°°ì—´ ì²˜ë¦¬
freq_array = np.array(list(notes.values()))
y_positions = hz_to_pixels(freq_array)
```

### time_to_pixels()

**Signature:**
```python
time_to_pixels(time_seconds: Union[float, np.ndarray, List[float]], 
               tempo: float = 120, 
               pixels_per_beat: float = 80) -> Union[float, np.ndarray]
```

ì‹œê°„(ì´ˆ)ì„ í”¼ì•„ë…¸ë¡¤ Xì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

**Parameters:**
- `time_seconds`: ì‹œê°„ ê°’ (ì´ˆ)
- `tempo`: í…œí¬ (BPM, ê¸°ë³¸ê°’: 120)
- `pixels_per_beat`: ë°•ìë‹¹ í”½ì…€ ìˆ˜ (ê¸°ë³¸ê°’: 80)

**Examples:**
```python
# ë‹¤ì–‘í•œ í…œí¬ì—ì„œì˜ ë³€í™˜
times = [0, 0.5, 1.0, 1.5, 2.0]

for tempo in [60, 120, 180]:
    x_coords = time_to_pixels(times, tempo=tempo)
    print(f"í…œí¬ {tempo}BPM: {x_coords}")

# ì¤Œ ë ˆë²¨ ë³€ê²½
x_normal = time_to_pixels(times, pixels_per_beat=80)   # ê¸°ë³¸
x_zoomed = time_to_pixels(times, pixels_per_beat=160)  # 2ë°° í™•ëŒ€
```

### create_pitch_curve_data()

**Signature:**
```python
create_pitch_curve_data(frequency: Union[np.ndarray, List[float]], 
                        time_seconds: Union[np.ndarray, List[float]],
                        tempo: float = 120,
                        pixels_per_beat: float = 80,
                        color: str = "#FF6B6B",
                        line_width: int = 3,
                        opacity: float = 0.8) -> Dict[str, Any]
```

ì™„ì „í•œ F0 ì»¤ë¸Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**Parameters:**
- `frequency`: ì£¼íŒŒìˆ˜ ë°°ì—´ (Hz)
- `time_seconds`: ì‹œê°„ ë°°ì—´ (ì´ˆ)
- `tempo`: í…œí¬ (BPM)
- `pixels_per_beat`: ë°•ìë‹¹ í”½ì…€ ìˆ˜
- `color`: ì»¤ë¸Œ ìƒ‰ìƒ (ê¸°ë³¸ê°’: "#FF6B6B")
- `line_width`: ì„  ë‘ê»˜ (ê¸°ë³¸ê°’: 3)
- `opacity`: íˆ¬ëª…ë„ 0-1 (ê¸°ë³¸ê°’: 0.8)

**Returns:**
- í”¼ì•„ë…¸ë¡¤ LineLayerìš© ì™„ì „í•œ ì»¤ë¸Œ ë°ì´í„° ë”•ì…”ë„ˆë¦¬

**Examples:**
```python
# ê¸€ë¼ì´ë“œ íš¨ê³¼ ìƒì„±
duration = 3.0
num_points = 300
times = np.linspace(0, duration, num_points)

# A4ì—ì„œ C5ë¡œ ë¶€ë“œëŸ½ê²Œ ìƒìŠ¹
start_freq, end_freq = 440.0, 523.25
frequencies = start_freq + (end_freq - start_freq) * (times / duration)

# ë¹„ë¸Œë¼í†  ì¶”ê°€
vibrato = 5 * np.sin(2 * np.pi * 6 * times)  # 6Hz ë¹„ë¸Œë¼í† 
frequencies += vibrato

# ì»¤ë¸Œ ë°ì´í„° ìƒì„±
pitch_curve = create_pitch_curve_data(
    frequencies, times,
    color="#FF4444",
    line_width=4,
    opacity=0.9
)

# ì—¬ëŸ¬ ì»¤ë¸Œ ì¡°í•©
curves = {
    "main_melody": pitch_curve,
    "harmony": create_pitch_curve_data(
        frequencies * 1.25,  # ì™„ì „ 4ë„ ìœ„
        times,
        color="#4444FF",
        line_width=2
    )
}
```

### create_loudness_curve_data()

**Signature:**
```python
create_loudness_curve_data(loudness: Union[np.ndarray, List[float]], 
                          time_seconds: Union[np.ndarray, List[float]],
                          tempo: float = 120,
                          pixels_per_beat: float = 80,
                          use_db: bool = True,
                          y_min: Optional[float] = None,
                          y_max: Optional[float] = None,
                          color: str = "#4ECDC4",
                          line_width: int = 2,
                          opacity: float = 0.6) -> Dict[str, Any]
```

ì™„ì „í•œ ë¼ìš°ë“œë‹ˆìŠ¤ ì»¤ë¸Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**Examples:**
```python
# RMS ì—ë„ˆì§€ ë¶„ì„
y, sr = librosa.load("audio.wav")
rms = librosa.feature.rms(y=y, hop_length=512)[0]
times = librosa.frames_to_time(np.arange(len(rms)), sr=sr)

# dB ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
rms_db = 20 * np.log10(np.maximum(rms/np.max(rms), 1e-6))

# ë¼ìš°ë“œë‹ˆìŠ¤ ì»¤ë¸Œ ìƒì„±
loudness_curve = create_loudness_curve_data(
    rms_db, times,
    use_db=True,
    y_min=-60,  # -60dB ìµœì†Œê°’
    y_max=0,    # 0dB ìµœëŒ€ê°’
    color="#00CCCC"
)

# ë‹¤ì¤‘ íŠ¹ì„± ë¶„ì„
spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
brightness_curve = create_loudness_curve_data(
    spectral_centroids, times,
    use_db=False,
    color="#FFAA00",
    line_width=1
)
```

### audio_array_to_base64_wav()

**Signature:**
```python
audio_array_to_base64_wav(audio_data: np.ndarray, 
                         sample_rate: int = 44100,
                         normalize: bool = True) -> str
```

numpy ì˜¤ë””ì˜¤ ë°°ì—´ì„ base64 WAV ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

**Examples:**
```python
# í•©ì„± ì˜¤ë””ì˜¤ ìƒì„±
duration = 2.0
sr = 44100
t = np.linspace(0, duration, int(duration * sr))

# A4 ì‚¬ì¸íŒŒ ìƒì„±
audio = 0.3 * np.sin(2 * np.pi * 440 * t)

# ADSR ì—”ë²¨ë¡œí”„ ì ìš©
fade_len = int(0.1 * sr)
audio[:fade_len] *= np.linspace(0, 1, fade_len)    # Attack
audio[-fade_len:] *= np.linspace(1, 0, fade_len)   # Release

# base64 ë³€í™˜
audio_base64 = audio_array_to_base64_wav(audio, sr)

# í”¼ì•„ë…¸ë¡¤ì— ì„¤ì •
piano_roll = PianoRoll(value={
    "notes": [],
    "audio_data": audio_base64,
    "use_backend_audio": True
})
```

## ğŸš€ librosa í†µí•© í•¨ìˆ˜

### librosa_f0_to_pixels()

librosa F0 ì¶”ì¶œ ê²°ê³¼ë¥¼ ì§ì ‘ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```python
import librosa
from gradio_pianoroll import librosa_f0_to_pixels

# F0 ì¶”ì¶œ
y, sr = librosa.load("vocal.wav", sr=22050)
f0, voiced_flag, voiced_probs = librosa.pyin(y, sr=sr, hop_length=512)

# ì§ì ‘ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
f0_x_pixels, f0_y_pixels = librosa_f0_to_pixels(
    f0, sr=sr, hop_length=512, tempo=120
)

print(f"ìœ íš¨í•œ F0 í¬ì¸íŠ¸: {len(f0_x_pixels)}ê°œ")
```

### librosa_rms_to_pixels()

librosa RMS ì¶”ì¶œ ê²°ê³¼ë¥¼ ì§ì ‘ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```python
# RMS ì¶”ì¶œ
rms = librosa.feature.rms(y=y, hop_length=512)[0]

# í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜ (dB ìŠ¤ì¼€ì¼)
rms_x_pixels, rms_y_pixels = librosa_rms_to_pixels(
    rms, sr=sr, hop_length=512, to_db=True
)
```

## ğŸ”§ ê³ ê¸‰ í™œìš© ì˜ˆì œ

### ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸

```python
def analyze_audio_realtime(audio_file, tempo=120):
    """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ë¥¼ í”¼ì•„ë…¸ë¡¤ë¡œ ì‹œê°í™”"""
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ
    y, sr = librosa.load(audio_file, sr=22050)
    
    # íŠ¹ì„± ì¶”ì¶œ
    f0, voiced_flag, voiced_probs = librosa.pyin(y, sr=sr)
    rms = librosa.feature.rms(y=y, hop_length=512)[0]
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    
    # ì‹œê°„ì¶•
    hop_length = 512
    times = librosa.frames_to_time(np.arange(len(f0)), sr=sr, hop_length=hop_length)
    
    # ì‹ ë¢°ë„ ë†’ì€ F0ë§Œ ì‚¬ìš©
    reliable_mask = voiced_flag & (f0 > 0) & (voiced_probs > 0.8)
    
    curves = {}
    
    # F0 ì»¤ë¸Œ
    if np.any(reliable_mask):
        curves["f0"] = create_pitch_curve_data(
            f0[reliable_mask], times[reliable_mask], 
            tempo=tempo, color="#FF6B6B"
        )
    
    # RMS ì»¤ë¸Œ (dB)
    rms_times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length)
    rms_db = 20 * np.log10(np.maximum(rms/np.max(rms), 1e-6))
    
    curves["loudness"] = create_loudness_curve_data(
        rms_db, rms_times, tempo=tempo,
        use_db=True, y_min=-40, y_max=0, color="#4ECDC4"
    )
    
    # MFCC ì²« ë²ˆì§¸ ê³„ìˆ˜ (spectral centroidì™€ ìœ ì‚¬)
    mfcc_times = librosa.frames_to_time(np.arange(mfccs.shape[1]), sr=sr, hop_length=hop_length)
    curves["timbre"] = create_loudness_curve_data(
        mfccs[1], mfcc_times, tempo=tempo,  # ë‘ ë²ˆì§¸ MFCC ê³„ìˆ˜
        use_db=False, color="#9B59B6", line_width=1
    )
    
    # ì˜¤ë””ì˜¤ ë°ì´í„°
    audio_base64 = audio_array_to_base64_wav(y, sr)
    
    return {
        "notes": [],
        "tempo": tempo,
        "line_data": curves,
        "audio_data": audio_base64,
        "use_backend_audio": True
    }
```

### AI ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬

```python
def process_ai_singing_synthesis(ai_f0_pred, ai_loudness_pred, phoneme_durations, tempo=120):
    """AI ë…¸ë˜ í•©ì„± ëª¨ë¸ì˜ ì¶œë ¥ì„ í”¼ì•„ë…¸ë¡¤ë¡œ ë³€í™˜"""
    
    # í”„ë ˆì„ì„ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (25ms í”„ë ˆì„ ê°€ì •)
    frame_duration = 0.025
    times = np.arange(len(ai_f0_pred)) * frame_duration
    
    # F0 í›„ì²˜ë¦¬ (í‰í™œí™”)
    from scipy import signal
    smoothed_f0 = signal.savgol_filter(ai_f0_pred, 15, 3)
    
    # ë³´ì´ì‹±ëœ êµ¬ê°„ë§Œ ì¶”ì¶œ (ì„ê³„ê°’ ê¸°ë°˜)
    voiced_mask = ai_f0_pred > 80  # 80Hz ì´ìƒ
    
    # ì»¤ë¸Œ ìƒì„±
    curves = {}
    
    if np.any(voiced_mask):
        curves["ai_f0"] = create_pitch_curve_data(
            smoothed_f0[voiced_mask], times[voiced_mask],
            tempo=tempo, color="#E74C3C", line_width=3
        )
    
    # ë¼ìš°ë“œë‹ˆìŠ¤ ì»¤ë¸Œ
    curves["ai_loudness"] = create_loudness_curve_data(
        ai_loudness_pred, times, tempo=tempo,
        use_db=False, color="#3498DB", line_width=2
    )
    
    # ìŒì†Œ ê²½ê³„ í‘œì‹œ (ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„°ë¡œ)
    segment_data = []
    current_time = 0
    
    for i, duration in enumerate(phoneme_durations):
        segment_data.append({
            "start": current_time,
            "end": current_time + duration,
            "type": "phoneme",
            "value": f"phoneme_{i}",
            "confidence": 1.0
        })
        current_time += duration
    
    return {
        "notes": [],
        "tempo": tempo,
        "line_data": curves,
        "segment_data": segment_data
    }
```

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
def batch_process_audio_files(file_list, batch_size=4, tempo=120):
    """ë‹¤ìˆ˜ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°°ì¹˜ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬"""
    
    all_results = {}
    
    for i in range(0, len(file_list), batch_size):
        batch_files = file_list[i:i+batch_size]
        
        # ë³‘ë ¬ ì²˜ë¦¬ (multiprocessing)
        from multiprocessing import Pool
        
        with Pool(processes=batch_size) as pool:
            batch_results = pool.map(
                lambda f: analyze_audio_realtime(f, tempo), 
                batch_files
            )
        
        # ê²°ê³¼ ì €ì¥
        for file_path, result in zip(batch_files, batch_results):
            filename = os.path.basename(file_path)
            all_results[filename] = result
    
    return all_results
```

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬

```python
# ëŒ€ìš©ëŸ‰ ë°°ì—´ ì²˜ë¦¬ ì‹œ
def process_large_f0_array(f0_array, times_array, chunk_size=10000):
    """ëŒ€ìš©ëŸ‰ F0 ë°°ì—´ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
    
    all_data_points = []
    
    for i in range(0, len(f0_array), chunk_size):
        chunk_f0 = f0_array[i:i+chunk_size]
        chunk_times = times_array[i:i+chunk_size]
        
        # ìœ íš¨í•œ ê°’ë§Œ ì²˜ë¦¬
        valid_mask = np.isfinite(chunk_f0) & (chunk_f0 > 0)
        if np.any(valid_mask):
            x, y = hz_and_time_to_pixels(
                chunk_f0[valid_mask], 
                chunk_times[valid_mask]
            )
            
            chunk_points = [
                {"x": float(x_val), "y": float(y_val)} 
                for x_val, y_val in zip(x, y)
            ]
            all_data_points.extend(chunk_points)
    
    return all_data_points
```

### GPU ê°€ì† (CuPy ì‚¬ìš©)

```python
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

def gpu_accelerated_conversion(frequencies, times):
    """GPUë¥¼ ì‚¬ìš©í•œ ê³ ì† ì¢Œí‘œ ë³€í™˜"""
    if not GPU_AVAILABLE:
        return hz_and_time_to_pixels(frequencies, times)
    
    # GPUë¡œ ë°ì´í„° ì „ì†¡
    freq_gpu = cp.asarray(frequencies)
    times_gpu = cp.asarray(times)
    
    # GPUì—ì„œ ê³„ì‚°
    midi_gpu = 69 + 12 * cp.log2(freq_gpu / 440.0)
    y_gpu = (128 - 1 - midi_gpu) * 20 + 10
    x_gpu = times_gpu * (120 / 60.0) * 80
    
    # CPUë¡œ ê²°ê³¼ ë°˜í™˜
    return cp.asnumpy(x_gpu), cp.asnumpy(y_gpu)
```

ì´ APIë¥¼ ì‚¬ìš©í•˜ë©´ ë³µì¡í•œ ì¢Œí‘œ ë³€í™˜ ë¡œì§ì„ ì‹ ê²½ ì“°ì§€ ì•Šê³ ë„ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ë¥¼ í”¼ì•„ë…¸ë¡¤ì—ì„œ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 